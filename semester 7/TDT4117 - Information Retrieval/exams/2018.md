# 1
## 1
Optimization and better results. A normal database will be able to search for documents to a certain degree, but is not made for that. The reason is that a database will not be smart enough to use a query well enough, as well as indexing all the documents. This means that search will be very slow and limited.

- mangel p√• rangering
- krav om fullstendig match

## 2
The concept of an IR system consists of multiple parts. It start in the UI where the user submits a query. This query is then transformed the same way as the documents in the document collection by using stemming, tokenizing etc. After this, the documents are indexed, and the query is expanded by finding synonyms, correcting spelling mistakes etc. Then, documents can be retrieved using a retrieval model on the query and the documents. Returned documents are ranked against each other and formatted before being sendt back to the user interface.

## 3
Stemming can help improve recall by normalizing the text. However, it is expected to lose some precision as a result.

# 2
## 1
A signature file is a file that contains the hashed version of the indexed terms. It is generated by allocating terms to blocks with a maximum number of terms per block equal to a set block size. A hash function is also needed in order to compute bitmasks for each term, that will then be combined by bitwise OR-ing each term in each block to determine the bitmask of each block. The bitmask of non-indexed blocks will be set to 0.
To search for a term, we would run the hash function on it, and use the resulting bitmask b. By applying it to each block using bitwise AND, we would expect to find a case where b & m = b. This means that the term could be inside the current block. By checking each term in the for equality using bitwise and, we expect one of them to return only 1's, where the term is found. If not, the block was a false drop.

## 2
lexical analysis (punctuation, case)
stopword removal
stemming
index term selection (tokenizing)
thesauri (used in query expansion)

compression

## 3

# 3
AP is the average of the calculated precisions for each retrieved document.
MAP is the mean of all the average precisions for each query result. This means it can be calculated by first getting the AP for each set, and then the mean of those combined. The notation "MAP@n" can be used to denote a specific MAP up to the n first documents. If not specified, MAP defaults to n being the highest rank number.

| Rank | q1 | P    |
|------|----|------|
| 1    | R  | 1/1  |
| 2    |    |      |
| 3    | R  | 2/3  |
| 4    |    |      |
| 5    |    |      |
| 6    | R  | 3/6  |
| 7    |    |      |
| 8    |    |      |
| 9    | R  | 4/9  |
| 10   | R  | 5/10 |

AP1=sigma(p)/10=0.62

| Rank | q2 | P    |
|------|----|------|
| 1    | R  | 1/1  |
| 2    | R  | 2/2  |
| 3    |    |      |
| 4    |    |      |
| 5    |    |      |
| 6    | R  | 3/6  |
| 7    |    |      |
| 8    |    |      |
| 9    |    |      |
| 10   | R  | 4/10 |
| 11   |    |      |
| 12   |    |      |
| 13   |    |      |
| 14   |    |      |
| 15   | R  | 5/15 |

AP2=sigma(p)/15=0.64

MAP=sigma(APi)/2=0.63

# 4
## 1

## 2
Vector space model uses vectors to compare a cosine difference that can be ranked. Okapi BM25 can also rank documents, but uses a probability instead. 

# 5
## 1
Distributed web search has the disadvantage of requiring the overhead coordination of gatherers and brokers. In addition, distributed web search is not very popularized, making it hard to maintain.

## 2
PageRank
Developed by google. Looks at number and quality of incoming links. Chance of someone randomly navigating to gives site.

HITS (hubs and authorities)
Hubs link to many sites.
Authorities are linked by many hubs.
Looks at whether a page is either a good hub or authority.

## 3
Can not be used because recall can not be calculated.

# 6
## 1
d

## 2
d

## 3
a

## 4
d

## 5
b

## 6
d

## 7
d

## 8
c

## 9
d

## 10
a