Øving 12 algoritmer og datastrukturer

Innholdsfortegnelse

Øving 12 algoritmer og datastrukturer
Innledning
Testfiler for komprimering
Krav til løsningen
Deloppgave Lempel-Ziv
Tips om Lempel-ziv
Filformat
Deloppgave Huffmankoding
Tips om Huffmankoding
Huffmanndata som trengs for å pakke ut igjen
Adaptiv Huffmannkoding
Om bitstrenger
Om koking
Javatips for begge deloppgaver
Noen kodeeksempler


  Innledning

Lag et program som kan lese en fil og lage en komprimert utgave. 
Lag et annet program som pakker ut igjen (dekomprimerer) og 
gjenskaper originalen. 

Bruk enten Huffmankoding eller Lempel-Ziv for å komprimere. 
Ressurssterke grupper med mange gode programmerere må gjerne lage 
begge deler for å oppnå best mulig kompresjon. I så fall 
anbefaler jeg å lage separate programmer, det gjør det enklere å 
teste og dele på arbeidet. Da går an å sjekke hvilken algoritme 
som komprimerer best, og om det er en fordel å komprimere output 
fra Lempel-Ziv med Huffmann.

Programmer som «zip» bruker Lempel-Ziv-Welsh for å komprimere, og 
deretter Huffmannkoding på output fra Lempel-Ziv.

Det kan bli en del arbeid, da håndtering av bits & bytes er nytt 
for mange. Det er derfor denne øvingen teller litt mer.

  Testfiler for komprimering

Oppgavetekst (pdf) http://www.iie.ntnu.no/fag/_alg/kompr/opg12.pdf

Oppgavetekst (txt) http://www.iie.ntnu.no/fag/_alg/kompr/opg12.txt

Forelesningen (pdf) http://www.iie.ntnu.no/fag/_alg/kompr/diverse.pdf

Forelesningen (txt) http://www.iie.ntnu.no/fag/_alg/kompr/diverse.txt

Forelesningen (lyx) http://www.iie.ntnu.no/fag/_alg/kompr/diverse.lyx

  Krav til løsningen

1. Implementer enten Lempel-Ziv eller Huffmannkoding. (Eller 
  begge deler, om dere har tid!) Andre algoritmer blir ikke 
  godkjent medmindre det er avtalt på forhånd. Lempel-Ziv-Welsh 
  (LZW) er en annen algoritme.

2. Dere må lage programmene selv, ikke noe «cut & paste» fra 
  nettet. Grupper som ikke kan forklare detaljer i programmet 
  sitt, får ikke godkjent denne oppgaven. Det er mye å lære av å 
  gjøre en slik oppgave, som en ikke får med seg med «cut & paste»
  . Både når det gjelder algoritmene, og generell programmering. 

3. Komprimering og utpakking skal skje i separate kjøringer. Det 
  er ikke greit å ha ett samleprogram som både gjør innpakking og 
  utpakking i en operasjon. Utpakking skal bare trenge den 
  komprimerte fila, ikke noen variabler/datastrukturer fra 
  innpakkinga. 

4. Programmene må lese og skrive filer. Altså ikke bare testdata 
  i en tabell. 

5. Utpakkingsprogrammet må produsere en fil som er identisk med 
  originalen. Men det skal ikke trenge tilgang på originalfilen, 
  bare den komprimerte filen. 

  Likhet kan testes med «diff» (linux) eller «fc» (windows)

6. Komprimering må klare å spare minst 10% i forhold til 
  originalen, for én av mine testfiler. Operativsystemet kan 
  fortelle hvor store filene er, i bytes.

7. Programmet bruker ikke hasmap/hashset e.l., som ikke er 
  nødvendig her. 

  Deloppgave Lempel-Ziv

Implementer en variant av Lempel-Ziv datakompresjon. (Men ikke 
Lempel-Ziv-Welsh)

Finn ut hvor mye programmet deres komprimerer testfilene mine. 
Det er ikke sikkert alle filtyper lar seg komprimere. Men for å 
få godkjent, må gruppa i det minste kunne komprimere en fil så 
den sparer 10%, og deretter pakke den ut igjen.

Gruppa må dessuten kunne forklare detaljene i programmene sine.

  Tips om Lempel-ziv

Normalt blir det veldig lite kompresjon på små filer. Bittesmå 
filer kan brukes for å finne feil i programmet, men for å teste 
kompresjon bør filene minst være på noen kilobyte.

Det blir noen avgjørelser å ta, som f.eks. hvor langt bakover 
programmet deres skal lete etter repeterte sekvenser. Zip leter 
32kB bakover, det fins også versjoner som går 64kB tilbake. Hvis 
dere lar programmet gå lenger tilbake, vil det bli tregere men 
sannsynligvis komprimere bedre også.

Om en vil ha et veldig kjapt program, kan det lønne seg å la seg 
inspirere av avanserte tekstsøkalgoritmer. 

  Filformat

Filformat bestemmer dere selv. Det kan fort bli en avveiing 
mellom hvor komplisert programmet skal være, og hvor godt det 
skal komprimere.

Den komprimerte fila kan bestå av blokker. Hver blokk starter med 
en byte-verdi, som er et tall mellom -128 og +127. Hvis tallet er 
negativt, f.eks. -57, betyr det at det er en serie med tegn som 
ikke lot seg komprimere. (I dette eksempelet, 57 tegn). 

Hvis tallet er positivt, angir det lengden på en repetert 
sekvens. De neste 1, 2 eller 4 byte er et heltall som forteller 
hvor langt bakover i fila denne sekvensen er å finne. Med 1 byte 
(byte) er det bare mulig å gå 127 tegn bakover. Programmet blir 
raskt, men komprimerer antagelig ikke så kraftig. Med 2 byte 
(short) går det an å gå opp til 32 kB bakover, men vi bruker 
altså opp en ekstra byte. Med 4 byte (int) kan vi gå opp til 2 GB 
bakover. Det gir mange flere muligheter for å finne repeterte 
strenger, men bruker også mer plass. Et program som leter opptil 
2 GB bakover, blir sannsynligvis temmelig tregt også. Det kan 
lønne seg å begrense litt…

  Deloppgave Huffmankoding

Lag et program som leser inn en fil og genererer en huffmanntre 
ut fra byte-verdiene i filen. Deretter bruker programmet 
huffmanntreet til å skrive en komprimert huffmannkodet fil. Sjekk 
hvor mye plass dere sparer, ved å komprimere testfilene mine. 
Dere må også kunne pakke filene ut igjen.

For pakke ut, trenger utpakkingsprogrammet nok informasjon til å 
gjenskape huffmantreet. Det enkleste er å legge frekvenstabellen 
først i den komprimerte fila. Adaptiv huffmankoding er en mer 
avansert og krevende løsning.

For å få godkjent, må ihvertfall en av filene komprimeres med 
minst 10%.

  Tips om Huffmankoding

  Huffmanndata som trengs for å pakke ut igjen

Det er ikke nødvendig å lagre huffmanntreet, det holder å lagre 
frekvenstabellen. Utpakkingsprogrammet kan dermed bygge opp samme 
tre ut fra frekvensene. 

int frekvenser[256];

En slik frekvenstabell blir alltid 1 kB, filen som skal 
komprimeres må dermed være stor nok til at komprimeringen sparer 
mer enn 1 kB.

  Adaptiv Huffmannkoding

Med adaptiv huffmannkoding slipper man å lagre frekvensene også. 
Man deler fila opp i blokker med fast størrelse. Første blokk 
komprimerer man ikke, den bare kopieres til output. Samtidig 
lager man et huffmanntre. Neste blokk komprimeres med 
huffmanntreet fra forrige blokk. Samtidig oppdaterer man 
frekvensene, og lager nytt huffmanntre som brukes for neste blokk 
osv. 

Adaptiv huffmankoding blir bedre, fordi den klarer å ta hensyn 
til at bokstavfordelingen endrer seg underveis.

  Om bitstrenger

En bitstreng er ikke en streng som dette: "00001101". Dette er en 
tekststreng med 8 tegn. Skriver vi dette til en fil, går det med 
8 byte, og vi oppnår ikke noe datakompresjon. Tvert imot får vi 
en veldig stor fil!

Men bitstrengen 0b00001101 er det samme som tallet 13, og kan 
lagres som én byte.

Datatypen «long» er på 64 bit. Ingen tegn vil trenge lenger 
Huffmankode enn det. (Det kan vises at nå man komprimerer en fil 
på 2.7GB, trenger ingen tegn kodes med mer enn 44 bit.) «long» er 
dermed egnet til å lagre bitstrenger. En «long» har alltid 64 
bit, så en bitstreng-klasse må også ha et felt som forteller hvor 
mange av bitene som er med i bitstrengen.

Å skrive bitstrenger til fil, blir en del ekstra arbeid. Java lar 
oss bare skrive hele byte, og for å være effektive bør vi bare 
skrive byte-array av en viss størrelse. Men, med 
høyre/venstreskift samt binære & og | -operasjoner, kan vi få 
våre bitstrenger inn i et byte-array som så kan skrives til disk. 

Tilsvarende for lesing: Vi leser inn et byte-array, og plukker 
deretter ut én og én bit for å navigere gjennom huffmanntreet.

  Om koking

På nettet fins mange implementasjoner av Huffmannkoding. De har 
sine særegenheter som vi kjenner igjen. Programmer som bruker 
hashset/hasmap vil bli underkjent som kok. hashopplegg trengs 
ikke for å løse denne oppgaven.

  Javatips for begge deloppgaver