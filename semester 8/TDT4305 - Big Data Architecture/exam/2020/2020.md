# 1
## 1
### 1.  Discuss the principle behind the handling of data streams in AsterixDB. How is this different from Spark and Storm?
Storm vs Spark:
![](images/2022-06-09-13-13-10.png)

AsterixDB vs. both:  
- Scalable, Fault-Tolerant, and Elastic data ingestion facility 
- Plug-and-Play model to cater to wide variety of data sources, through both UDF and 
Adapters. 
- While Storm & Spark are both with custom built solution, e.g, Storm + MongoDB, 
AsterixDB can reduce the number of moving parts needed to handle Big Data.


AsterixDB pros/cons:
+ Et komplett system med innebygd effektiv lagringshåndtering av big data som også kan 
utvides med eksisterende lagringssystemer 
+ Sofistikerte «recovery»-mekanismer som kan håndtere både «soft og hard failures» 
+ Tillater å ta inn høyhastighetsdata med effektiv håndtering 

+ Lar man håndtere data både i batch og ren strøm (sanntidsbehandling/analyse) form 
+ Feed kan programmeres til å håndtere én input-strøm og produsere flere strøm av resultat-
outputs («fetch once, compute many» prinsippet). 
- Komplisert – kan være for mange moving parts, noe som gjør det vanskelig å vedlikeholde 
effektivt 
- Proprietære spørringsspråk 
- Ikke like utbredt som Storm og Spark

asterix:
lappeteppe med flere deler
one size fits a bunch
data-sensitiv beregning
parallelle databasesystemer
semistrukturert datahåndtering
system for dataintensiv beregning
less moving parts for user
harder to maintain for asterix developers


### 2.  There exist three different delivery semantics or message guarantees. Explain what these three guarantees mean in practice. Make a table explaining which guarantee(s) AsterixDB, Spark and Storm are meant to meet, respectively. For this, you will use the following template:
- messages sent at most once
    - means it will not be retransmitted after received
    - may cause data loss
    - for less important data
- at least once
    - means it can be retransmitted until processing is complete
    - may duplicate data
        - system needs to be duplication tolerant
            - idempotent (processing again does not impact system (e.g. upsert - insert first, update after))
    - for more important data
    - most common
- exactly once
    - assuming data will not get lost
    - difficult to achieve, rarely used

|                    | AsterixDB     | Storm                        | Spark        |
|--------------------|---------------|------------------------------|--------------|
| delivery guarantee | at least once | at most once / at least once | exactly once |
| example            |               |                              |              |

### 3 Explain the following figure of a Storm system
The stream starts with a spout, which is a stream source, e.g. an IG feed. This stream goes through several bolts which are funcitons that produce new streams by transforming the content in a certain way. This then feeds into another spout that merges the stream with a new stream source. This is finally fed into two more bolts in the same way as before, before the stream pipeline completes.

# 2
## 1.  Explain how you would solve this problem using standing queries.
This could be solved by asking how many times each user has visited the web page over a given time period. 
This could be solved by continuously asking if there is a new visit from each user, which would be added to the total number of visits for the given user. Meaning each user would have their own counter assigned to them.
## 2.  Could this problem be solved using bit counting? Justify your answer.
This could be solved using bit counting by using a time-based window. The bit would the be 1 each time the user visits the website, and 0 otherwise.

## 3
A bucket representation of a stream is a segment of a stream window, represented by a 
record consisting of:  
- The timestamp of its end [O(log N) bits]. 
- The number of 1’s between its beginning and end, which also constitutes the size of 
the bucket. 



## 3.  Assume you will use buckets to solve this problem. How does the concept of buckets work? How would you solve this problem using the principle of buckets?
The concept of buckets works in this case by storing records based on a bit stream. They each consist of:
- the timestamp of its end
- the number of 1's within the substring of bits
There are constraints on the buckets within this method, where the number of 1's must be a power of 2, and the number of 1's within each bucket must be sorted. 
This allows for fast bit counting, but with accuracy limited to 50%.


# 3
## 1
**content-based recommendation**
**should be used** if the recommender system should:
- recommend to users with unique tastes. This is because specific features of an item can be targeted, as opposed to collaborative filtering where an approximation is made by comparing towards other users that may not have the same specific tastes.
- not rely on data from other users. this can solve gdpr problems as well as the cold start problem. It will then also be able to recommend new and unpopular items.
- provide explanations for recommendations. can solve gdpr problems.

**Should not be used** if the system:
- finding appropriate features is hard, e.g. movies, images, music, where features may need to be exctracted using complex AI.
- need to recommend to new users, as building user profiles need to be buildt by collecting user data.
- recommending outside user profile is needed 
    - (often a problem as this may cause a feedback loop if the user does not explore by himself) 
    - people might have multiple interests 
- using quality judgement from others is important

**collaborative filtering** 
**should be used** if the recommender system should:
- work for any item without feature exctraction
- need especially specific recommendations (because features are not extracted)
- recommend different content not seen similar to before


**should not be used** if the system:
- should make specific recommendations (popularity bias)
- does not have enough users from the start (cold start)
- has a sparse user/ratings matrix (if users rarely rate items)
- does not have many first raters

## 2
| User-ID/Movie-ID | M1 | M2 | M3 | M4 | M5 | pearson(Ui, U4) (user-user) |
|------------------|----|----|----|----|----|-----------------------------|
| U1               | 7  | 6  | 7  | 4  | 5  |                             |
| U2               | 6  | 7  | 0  | 4  | 3  |                             |
| U3               | 0  | 3  | 3  | 1  | 1  |                             |
| U4               | 1  | 2  | 2  | 3  | 3  |                             |
| U5               | 1  | 0  | 1  | 2  | 3  |                             |

comparing users:

4-1
m4=sum([1, 2, 2, 3, 3])/5=2.2
m1=sum([7, 6, 7, 4, 5])/5=5.8
r4=[1-2.2, 2-2.2, 2-2.2, 3-2.2, 3-2.2]
r1=[7-5.8, 6-5.8, 7-5.8, 4-5.8, 5-5.8]
cos(r1,r2)=-0.871

4-2
m4=sum([1, 2, 2, 3, 3])/5=2.2
m2=sum([6, 7, 0, 4, 3])/5=4.0
r4=[1-2.2, 2-2.2, 2-2.2, 3-2.2, 3-2.2]
r2=[6-4.0, 7-4.0, 0-4.0, 4-4.0, 3-4.0]
cos(r1,r2)=-0.327

4-3
m4=sum([1, 2, 2, 3, 3])/5=2.2
m3=sum([0, 3, 3, 1, 1])/5=1.6
r4=[1-2.2, 2-2.2, 2-2.2, 3-2.2, 3-2.2]
r3=[0-1.6, 3-1.6, 3-1.6, 1-1.6, 1-1.6]
cos(r1,r2)=0.089

4-5
m4=sum([1, 2, 2, 3, 3])/5=2.2
m5=sum([1, 0, 1, 2, 3])/5=1.4
r4=[1-2.2, 2-2.2, 2-2.2, 3-2.2, 3-2.2]
r5=[1-1.4, 0-1.4, 1-1.4, 2-1.4, 3-1.4]
cos(r1,r2)=0.681


# 4
## 1
writing to a file starts with the client accessing a file through the name node. The client asks, then writes changes through this node, which then translates the changes into creation/replication/deletion of blocks sent to the data node. The data node will in the case of writing be commanded to write to all replicated data nodes, directly to the first data node, then through chained redirects to the next nodes. The datanodes will then report back to the name node when all the data is written. New blocks will be created and replicated (according to the block replication level) if needed.

## 2
There are two types of transformations, the first one is narrow transformations. In narrow transformations, all data that is needed to compute the records in a partition resides within that same partition. The second type is wide transformations, which means that the data needed to compute the records in a single partition might reside across several different partitions of the parent-RDD. Examples of narrow transformations are map() and filter(), and examples of wide transformations are groupbyKey() and reducebyKey(). 

Wide transforations are expensive when compared to narrow ones. This is because they involve a process called shuffling. This process occurs when the data in a single partitions may live in many partitions of the parent RDD. This means that the data is shuffled between partitions during the transformation, which is costly.

# 5

**shingles** are a form of decomposing and re-combining words in different ways in order to try to find which items are similar to each other. The decompositioning is done by splitting up tokens (words) into sequences of k letters. This is called k-shingling where a number k is selected, and used to construct all combinations of words from a document.

**minhashing**
A technique for quickly estimating how similar two sets are. It uses minhashing function(s) to compute the minimum hash value for each tokenized item in each set using each hash function.

The algorithm iterates through tokens in all the sets and hashes only the ones containing a bit = 1. This is permutated until there are no more tokens to be hashed. When finished, the sets will each have their own signature. These can be further analyzed for similarity using LSH.

**LSH**
Hensikt: Effektivt finne dokument-par som er potensielle "nær-duplikat", alternativet er f.eks. å sjek-
ke alle par av dokument (evt. signaturane deira) mot kvarandre.  
Input: Signatur-matrise M (Docs x Signatures), Output: Liste med kandidat-par S1, S2 som må evalu-
erast med s(S1, S2) 
Datastruktur: Matrisa M er delt inn i b "bands", kvar med r rader:
![](images/2022-06-09-13-59-40.png)
For kvart dokument, for kvar band: hash denne delen av signaturen inn i hash-bøtte (Separate bøtter 
for kvart band). 
Gå gjennom alle bøtter for alle band: Konstruer kandidat-par for alle dokument i bøtte. 

# 6
Balance:
Given advertisers {a1,a2,a3...} with budgets {b1,b2,b3...} along with their bids for each query, and a list of queries [q1,q2,q3]
- for each query q, pick the advertiser a with largest unspent budget b that also can afford the bid
- break ties arbitrarily, but deterministically

greedy:
For a query pick any advertiser who has bid for that query

Generalized Balance:
pick based on score based on bidf