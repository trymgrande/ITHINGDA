# 1
## 1

## 2
- hadoop spark
- storm


# 2
## 1
big data framework that builds on hdfs and can utilize hadoop mapreduce and/or a nosql database on top. it is also common to combine it with apache spark. hdfs (hadoop distributed file system) is the distributed file system within the hadoop eco system. It works using distribution where blocks are spread across multiple computers, with file pointers to the correct computer. This distribution also implements replication, meaning the data is always backed up, and will also allow for faster reading by having multiple servers to choose the closest one from. It can store huge files, but with slow access, and is popularized due to its ability to work using normal off the shelf hw. It is inspired by google fs, and made for data stream access, rather than random access. The blocks are stored as normal local files, where files less than a block size still only use the actual file size on disk. 
The nodes are separated into namenodes and datanodes. namenodes are master daemons that store metadata about files, while datanodes are slave daemons that store the actual file blocks. 

## 2
namenodes are responsible for storing the metadata about a given file, such as filename, path, block level, id, (physical?) location, replicas etc. It also keeps track of all the data nodes as a master daemon by sending commands like create/replicate/delete certain blocks. It also listens for hearthbeat commands to make sure each datanode is working correctly.

## 3
error tolerance is handled by replication. The namenode makes sure that there are always a n-way replication of the datanodes, so the datanodes therefore don't need to think about replication. fault detection is done by allowing the NameNode to check the checksum in the DataNode for corruption, and by listening for missing hearthbeats.

# 3
## 1
idea behind mapreduce is to be able to execute certain tasks much quicker by utilizing multiple computers at once. This is a central concept within the hadoop ecosystem as it is based around using many "normal" computers rather than few "supercomputers". This works by mapping e.g. groups of items in a set to their own computer. Then, the data gets shuffled by moving key-value pairs to each computer, which are then aggregated. This process allows for parallell computing by splitting up the problem into smaller segments.

## 2
map:
1
in
Adrian: Bjørn, Camilla, David
out
adrian,bjørn: camilla, david
adrian,camilla: bjørn, david
adrian,david: bjørn, camilla


2
Bjørn: Adrian, Camilla, David, Ester
out
adrian,bjørn: camilla, david, ester
bjørn,camilla: david,ester
bjørn,david: adrian,camilla,ester
...

reduce
adrian,bjørn: camilla, david
adrian,bjørn: camilla, david, ester
adrian,camilla: bjørn,david

adrian,bjørn: camilla, david
adrian,camilla: bjørn,david

## 3
map((x,y => (
    (x,y1: y2,y3),
    (x,y2: y1,y3),
    (x,y3: y1,y2)
))

reduce((x,y) => (
    reduce(x2, y2)
)


# 4

## 2
- master-slave model where processes are coordinated through zookeeper
- components
    - nimbus
        - master that distributes application code across worker nodes (similar to hadoop)
        - stateless single node
            - fail-fast unlike hadoop
                - can be restarted without recovery
    - supervisor nodes
        - worker
        - runs daemon (process) responsible for creating, starting, stopping worker processes to execute assigned tasks
        - fail-fast
            - stores state in zookeeper
    - zookeeper
        - cordinates between the two
        - all data (state) is stored here


## 3








# 5
## 3
| edges | edge betweenness |
|-------|------------------|
| AD    | 3+3+0+2+1+1=10   |
| DF    | 1+1+0+1+0+2=5    |
| DE    | 1+1+1+2+2+0=7    |
| EC    | 0+1+3+1+2+2=9    |
| CB    | 1+2+2+0+1+1=7    |
| AB    | 2+4+1+1+0+0=8    |
| FE    | 0+0+1+0+1+3=5    |

