{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"85d8965a-c0b4-4802-abb8-05fc5e21ce59","showTitle":false,"title":""}},"source":["#### Names of people in the group"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"37c623c5-86c0-48ee-85de-5d514b4b334f","showTitle":false,"title":""}},"source":["Thomas Bjerke\n","\n","Trym Grande"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"324eb902-51f2-44b0-8ff1-730efac9900c","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["Out[98]: True"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Out[98]: True","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["# Deleting tables left from previous runs in case they still exist after deleting an inactive cluster\n","dbutils.fs.rm(f\"dbfs:/FileStore/dataframes\", recurse=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"60b530d6-b580-4de2-affb-aad878f4da86","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["\u001b[33mWARNING: You are using pip version 21.0.1; however, version 22.0.3 is available.\r\n","You should consider upgrading via the '/databricks/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"\u001b[33mWARNING: You are using pip version 21.0.1; however, version 22.0.3 is available.\r\nYou should consider upgrading via the '/databricks/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["# We need to install 'ipython_unittest' to run unittests in a Jupyter notebook\n","!pip install -q ipython_unittest"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"023f4d10-e729-450c-8d1a-50d494b488d6","showTitle":false,"title":""}},"outputs":[],"source":["# Loading PySpark modules that we need\n","import unittest\n","from collections import Counter\n","from pyspark.sql import DataFrame\n","from pyspark.sql.types import *\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"fcda919c-b51e-4b61-9d61-80cc24f2d15e","showTitle":false,"title":""}},"source":["#### Subtask 1: defining the schema for the data\n","Typically, the first thing to do before loading the data into a Spark cluster is to define the schema for the data. Look at the schema for 'badges' and try to define the schema for other tables similarly."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"ab8bdba5-f6d6-43bf-bed9-763d99cfcc91","showTitle":false,"title":""}},"outputs":[],"source":["# Defining a schema for 'badges' table\n","badges_schema = StructType([StructField('UserId', IntegerType(), False),\n","                            StructField('Name', StringType(), False),\n","                            StructField('Date', TimestampType(), False),\n","                            StructField('Class', IntegerType(), False)])\n","\n","# Defining a schema for 'posts' table\n","posts_schema = StructType([StructField('Id', IntegerType(), False),\n","                           StructField('ParentId', IntegerType(), True),\n","                           StructField('PostTypeId', IntegerType(), False),\n","                           StructField('CreationDate', TimestampType(), False),\n","                           StructField('Score', IntegerType(), False),\n","                           StructField('ViewCount', IntegerType(), False),\n","                           StructField('Body', StringType(), False),\n","                           StructField('OwnerUserId', IntegerType(), False),\n","                           StructField('LastActivityDate', TimestampType(), False),\n","                           StructField('Title', StringType(), True),\n","                           StructField('Tags', StringType(), True),\n","                           StructField('AnswerCount', IntegerType(), True),\n","                           StructField('CommentCount', IntegerType(), False),\n","                           StructField('FavoriteCount', IntegerType(), True),\n","                           StructField('CloseDate', TimestampType(), True)])\n","\n","# Defining a schema for 'users' table\n","users_schema = StructType([StructField('Id', IntegerType(), False),\n","                           StructField('Reputation', IntegerType(), False),\n","                           StructField('CreationDate', TimestampType(), False),\n","                           StructField('DisplayName', StringType(), False),\n","                           StructField('LastAccessDate', TimestampType(), False),\n","                           StructField('AboutMe', StringType(), True),\n","                           StructField('Views', IntegerType(), False),\n","                           StructField('UpVotes', IntegerType(), False),\n","                           StructField('DownVotes', IntegerType(), False)])\n","\n","# Defining a schema for 'comments' table\n","comments_schema = StructType([StructField('PostId', IntegerType(), False),\n","                            StructField('Score', IntegerType(), False),\n","                            StructField('Text', StringType(), False),\n","                            StructField('CreationDate', TimestampType(), False),\n","                            StructField('UserId', IntegerType(), False)])\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"fab2cfe2-0961-4a22-8fb1-9a6f9191fbcf","showTitle":false,"title":""}},"source":["#### Subtask 2: implementing two helper functions\n","Next, we need to implement two helper functions:\n","1. 'load_csv' that as input argument receives path for a CSV file and a schema and loads the CSV pointed by the path into a Spark DataFrame and returns the DataFrame;\n","2. 'save_df' receives a Spark DataFrame and saves it as a Parquet file on DBFS.\n","\n","Note that the column separator in CSV files is TAB character ('\\t') and the first row includes the name of the columns. \n","\n","BTW, DBFS is the name of the distributed filesystem used by Databricks Community Edition to store and access data."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"089f87ff-f2c8-4ac8-8449-cec251c502f6","showTitle":false,"title":""}},"outputs":[],"source":["def load_csv(source_file: \"path for the CSV file to load\", schema: \"schema for the CSV file being loaded as a DataFrame\") -> DataFrame:\n","    df_with_schema = spark.read.format(\"csv\") \\\n","      .option(\"header\", True) \\\n","      .option(\"delimiter\",\"\\t\") \\\n","      .schema(schema) \\\n","      .load(source_file)\n","    print(df_with_schema)\n","    return df_with_schema\n","\n","def file_exists(path):\n","  try:\n","    dbutils.fs.ls(path)\n","    return True\n","  except:\n","    return False\n","  \n","def save_df(df: DataFrame, table_name) -> bool:\n","    if file_exists(f\"dbfs:/FileStore/dataframes/{table_name}\"):\n","      return false\n","    df.write.parquet(f\"dbfs:/FileStore/dataframes/{table_name}\")\n","    return True"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"39bc683c-b37a-4842-8bf8-004620b17cca","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["The ipython_unittest extension is already loaded. To reload it, use:\n","  %reload_ext ipython_unittest\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"The ipython_unittest extension is already loaded. To reload it, use:\n  %reload_ext ipython_unittest\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["# Loading 'ipython_unittest' so we can use '%%unittest_main' magic command\n","%load_ext ipython_unittest"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"8903e903-7e4f-4c15-99bd-c9129a601fde","showTitle":false,"title":""}},"source":["#### Subtask 3: validating the implementation by running the tests\n","\n","Run the cell below and make sure that all the tests run successfully. Moreover, at the end there should be four Parquet files named 'badges', 'comments', 'posts', and 'users' in '/user/hive/warehouse'.\n","\n","Note that we assumed that the data for the project has already been stored on DBFS on the '/FileStore/tables/' path. (I mean as 'badges_csv.gz', 'comments_csv.gz', 'posts_csv.gz', and 'users_csv.gz'.)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"cd470d59-2571-4b7d-b022-9ee8f7c3e281","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["DataFrame[UserId: int, Name: string, Date: timestamp, Class: int]\n","DataFrame[PostId: int, Score: int, Text: string, CreationDate: timestamp, UserId: int]\n","DataFrame[Id: int, ParentId: int, PostTypeId: int, CreationDate: timestamp, Score: int, ViewCount: int, Body: string, OwnerUserId: int, LastActivityDate: timestamp, Title: string, Tags: string, AnswerCount: int, CommentCount: int, FavoriteCount: int, CloseDate: timestamp]\n","DataFrame[Id: int, Reputation: int, CreationDate: timestamp, DisplayName: string, LastAccessDate: timestamp, AboutMe: string, Views: int, UpVotes: int, DownVotes: int]\n","DataFrame[Id: int, Reputation: int, CreationDate: timestamp, DisplayName: string, LastAccessDate: timestamp, AboutMe: string, Views: int, UpVotes: int, DownVotes: int]\n","DataFrame[UserId: int, Name: string, Date: timestamp, Class: int]\n","DataFrame[PostId: int, Score: int, Text: string, CreationDate: timestamp, UserId: int]\n","DataFrame[Id: int, ParentId: int, PostTypeId: int, CreationDate: timestamp, Score: int, ViewCount: int, Body: string, OwnerUserId: int, LastActivityDate: timestamp, Title: string, Tags: string, AnswerCount: int, CommentCount: int, FavoriteCount: int, CloseDate: timestamp]\n","Success.....\n","----------------------------------------------------------------------\n","Ran 5 tests in 39.912s\n","\n","OK\n","Out[104]: <unittest.runner.TextTestResult run=5 errors=0 failures=0>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"DataFrame[UserId: int, Name: string, Date: timestamp, Class: int]\nDataFrame[PostId: int, Score: int, Text: string, CreationDate: timestamp, UserId: int]\nDataFrame[Id: int, ParentId: int, PostTypeId: int, CreationDate: timestamp, Score: int, ViewCount: int, Body: string, OwnerUserId: int, LastActivityDate: timestamp, Title: string, Tags: string, AnswerCount: int, CommentCount: int, FavoriteCount: int, CloseDate: timestamp]\nDataFrame[Id: int, Reputation: int, CreationDate: timestamp, DisplayName: string, LastAccessDate: timestamp, AboutMe: string, Views: int, UpVotes: int, DownVotes: int]\nDataFrame[Id: int, Reputation: int, CreationDate: timestamp, DisplayName: string, LastAccessDate: timestamp, AboutMe: string, Views: int, UpVotes: int, DownVotes: int]\nDataFrame[UserId: int, Name: string, Date: timestamp, Class: int]\nDataFrame[PostId: int, Score: int, Text: string, CreationDate: timestamp, UserId: int]\nDataFrame[Id: int, ParentId: int, PostTypeId: int, CreationDate: timestamp, Score: int, ViewCount: int, Body: string, OwnerUserId: int, LastActivityDate: timestamp, Title: string, Tags: string, AnswerCount: int, CommentCount: int, FavoriteCount: int, CloseDate: timestamp]\nSuccess.....\n----------------------------------------------------------------------\nRan 5 tests in 39.912s\n\nOK\nOut[104]: <unittest.runner.TextTestResult run=5 errors=0 failures=0>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["%%unittest_main\n","class TestTask1(unittest.TestCase):\n","   \n","    # test 1\n","    def test_load_badges(self):\n","        result = load_csv(source_file=\"/FileStore/tables/badges_csv.gz\", schema=badges_schema)\n","        self.assertIsNotNone(result, \"Badges dataframe did not load successfully\")\n","        self.assertIsInstance(result, DataFrame, \"Result type is not of spark.sql.DataFrame\")\n","        self.assertEqual(result.count(), 105640, \"Number of records is not correct\")\n","\n","        coulmn_names = Counter(map(str.lower, ['UserId', 'Name', 'Date', 'Class']))\n","        self.assertCountEqual(coulmn_names, Counter(map(str.lower, result.columns)),\n","                              \"Missing column(s) or column name mismatch\")\n","    \n","    # test 2\n","    def test_load_posts(self):\n","        result = load_csv(source_file=\"/FileStore/tables/posts_csv.gz\", schema=posts_schema)\n","        self.assertIsNotNone(result, \"Posts dataframe did not load successfully\")\n","        self.assertIsInstance(result, DataFrame, \"Result type is not of spark.sql.DataFrame\")\n","        self.assertEqual(result.count(), 61432, \"Number of records is not correct\")\n","\n","        coulmn_names = Counter(map(str.lower,\n","                                   ['Id', 'ParentId', 'PostTypeId', 'CreationDate', 'Score', 'ViewCount', 'Body', 'OwnerUserId',\n","                                    'LastActivityDate', 'Title', 'Tags', 'AnswerCount', 'CommentCount', 'FavoriteCount',\n","                                    'CloseDate']))\n","        self.assertCountEqual(coulmn_names, Counter(map(str.lower, result.columns)),\n","                              \"Missing column(s) or column name mismatch\")\n","    \n","    # test 3\n","    def test_load_comments(self):\n","        result = load_csv(source_file=\"/FileStore/tables/comments_csv.gz\", schema=comments_schema)\n","        self.assertIsNotNone(result, \"Comments dataframe did not load successfully\")\n","        self.assertIsInstance(result, DataFrame, \"Result type is not of spark.sql.DataFrame\")\n","        self.assertEqual(result.count(), 58735, \"Number of records is not correct\")\n","\n","        coulmn_names = Counter(map(str.lower, ['PostId', 'Score', 'Text', 'CreationDate', 'UserId']))\n","        self.assertCountEqual(coulmn_names, Counter(map(str.lower, result.columns)),\n","                              \"Missing column(s) or column name mismatch\")\n","    \n","    # test 4\n","    def test_load_users(self):\n","        result = load_csv(source_file=\"/FileStore/tables/users_csv.gz\", schema=users_schema)\n","        self.assertIsNotNone(result, \"Users dataframe did not load successfully\")\n","        self.assertIsInstance(result, DataFrame, \"Result type is not of spark.sql.DataFrame\")\n","        self.assertEqual(result.count(), 91616, \"Number of records is not correct\")\n","\n","        coulmn_names = Counter(map(str.lower,\n","                                   ['Id', 'Reputation', 'CreationDate', 'DisplayName', 'LastAccessDate', 'AboutMe',\n","                                    'Views', 'UpVotes', 'DownVotes']))\n","        self.assertCountEqual(coulmn_names, Counter(map(str.lower, result.columns)),\n","                              \"Missing column(s) or column name mismatch\")\n","    # test 5\n","    def test_save_dfs(self):\n","        dfs = [(\"/FileStore/tables/users_csv.gz\", users_schema, \"users\"),\n","               (\"/FileStore/tables/badges_csv.gz\", badges_schema, \"badges\"),\n","               (\"/FileStore/tables/comments_csv.gz\", comments_schema, \"comments\"),\n","               (\"/FileStore/tables/posts_csv.gz\", posts_schema, \"posts\")\n","               ]\n","\n","        for i in dfs:\n","            df = load_csv(source_file=i[0], schema=i[1])\n","            save_df(df, i[2])"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"0f99b257-8618-4796-aeb0-d9446863c259","showTitle":false,"title":""}},"source":["#### Subtask 4: answering to questions about Spark related concepts\n","\n","Please write a short description for the terms below---one to two short paragraphs for each term. Don't copy-paste; instead, write your own understanding.\n","\n","1. What do the terms 'Spark Application', 'SparkSession', 'Transformations', 'Action', and 'Lazy Evaluation' mean in the context of Spark?\n","\n","Write your descriptions in the next cell."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"91ec9fda-7848-4f01-8a36-3b82b78be007","showTitle":false,"title":""}},"source":["# Descriptions\n","\n","## Spark Application\n","A Spark application runs on a set of nodes that are clustered together. On one of these nodes sits the driver process, and the rest of the nodes consist of executor processes. The driver process is a kind of administrative process that keeps track of information about the application, as well as distributing work to the executor nodes. When the work is done, it is also the driver process' responsibility to respond to the user program. The executor processes do the actual computations, and report back to the driver process before the driver can report back to the user program. \n","\n","## SparkSession\n","A SparkSession is a way of unifying all of the contexts needed as entry points to utilize Spark's functionality. Without SparkSessions, developers would need to pay attention to all of the different contexts such as Hive contexts, SQL contexts etc. to run a spark application. What a SparkSession does is to gather all these contexts in to one, and thus make it way easier for developers to work with. The SparkSession object resides in the driver process and helps coordinate the executor nodes by keeping information about all of the contexts in the application. \n","\n","## Transformations\n","To understand this concept, one first has to know about RDDs. An RDD (Resilient Distributed Dataset) is Sparks fundemental datastructure, and are collections of any types of objects. They are records of data that is partitioned and replicated across multiple nodes (making them resilient/fault tolerant). A transformation is one of two operations that can be done on RDDs. A transformation means to take a RDD as input, do something with it, and then store the results in a new RDD. RDDs are immutable, which means that every time a transformation occurs, a new RDD is created so that the input-RDD is still the same as before the transform-operation. \n","\n","There are two types of transformations, the first one is narrow transformations. In narrow transformations, all data that is needed to compute the records in a partition resides within that same partition. The second type is wide transformations, which means that the data needed to compute the records in a single partition might reside across several different partitions of the parent-RDD. Examples of narrow transformations are map() and filter(), and examples of wide transformations are groupbyKey() and reducebyKey(). \n","\n","## Action\n","Actions are methods within an RDD (Resilient Distributed Dataset). These operations will be able to start a job that will  execute on a cluster. These can be implemented directly into the code, as long as the result set being worked on is small enough to fit into memory. Otherwise, it is also possible to write the data to the DBFS storage between execution. Whenever an action is called, the related transformations will also be executed. Common examples of these are 'reduce', 'collect', 'takeSample', 'take', 'first', 'saveAsTextfile', 'saveAsSequenceFile', 'countByKey', and 'Foreach'.\n","\n","## Lazy Evaluation\n","Lazy evaluation is a consept within Apache Spark. Here, the RDD can be considered the data. When a transformation is being called, it will only be appended to a transformation log called the DAG. This will allow many transformations to be called without them actually executing, because they are lazy. The transformations will only apply all at once whenever an action is executed."]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"Task1","notebookOrigID":118798763941640,"widgets":{}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
