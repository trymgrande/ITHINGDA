{"cells":[{"cell_type":"markdown","source":["#### Names of people in the group"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83045409-70a5-4010-b1cf-9db8d98f2bad"}}},{"cell_type":"markdown","source":["Thomas Bjerke\n\nTrym Grande"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"059a269d-8f13-494e-84e5-9da5d873047b"}}},{"cell_type":"code","source":["# We need to install 'ipython_unittest' to run unittests in a Jupyter notebook\n!pip install -q ipython_unittest"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"52939e3d-35c7-4767-a59e-ba75f716e952"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\u001B[33mWARNING: You are using pip version 21.0.1; however, version 22.0.3 is available.\r\nYou should consider upgrading via the '/databricks/python3/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[33mWARNING: You are using pip version 21.0.1; however, version 22.0.3 is available.\r\nYou should consider upgrading via the '/databricks/python3/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Loading modules that we need\nimport unittest\nfrom pyspark.sql.dataframe import DataFrame\nfrom typing import Any"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"499e46f5-12a4-46de-a212-90f2e936461e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# A helper function to load a table (stored in Parquet format) from DBFS as a Spark DataFrame \ndef load_df(table_name: \"name of the table to load\") -> DataFrame:\n    return spark.read.parquet(table_name)\n\nusers_df = load_df(\"dbfs:/FileStore/dataframes/users\")\ncomments_df = load_df(\"dbfs:/FileStore/dataframes/comments\")\nposts_df = load_df(\"dbfs:/FileStore/dataframes/posts\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f8c68e0-6bda-40a6-8588-381e51dc0a93"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Subtask 1: implenenting two helper functions\nImpelment these two functions:\n1. 'run_query' that gets a Spark SQL query and run it on df which is a Spark DataFrame; it returns the content of the first column of the first row of the DataFrame that is the output of the query;\n2. 'run_query2' that is similar to 'run_query' but instead of one DataFrame gets two; it returns the content of the first column of the first row of the DataFrame that is the output of the query.\n\nNote that the result of a Spark SQL query is itself a Spark DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a2274f0-1bd8-4812-83d2-f793587e9548"}}},{"cell_type":"code","source":["def run_query(query: \"a SQL query string\", df: \"the DataFrame that the query will be executed on\") -> Any:\n    df.createOrReplaceTempView(\"df\")\n    sql_df = spark.sql(query)\n    sql_df.show()\n    print(sql_df.head()[0])\n    return sql_df.head()[0]\n    \ndef run_query2(query: \"a SQL query string\", df1: \"DataFrame A\", df2: \"DataFrame B\") -> Any:\n    df1.createOrReplaceTempView(\"df1\")\n    df2.createOrReplaceTempView(\"df2\")\n    sql_df = spark.sql(query)\n    sql_df.show()\n    print(sql_df.head()[0])\n    return sql_df.head()[0]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f5fce811-7dd2-4602-a243-18a36754c302"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Loading 'ipython_unittest' so we can use '%%unittest_main' magic command\n%load_ext ipython_unittest"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2a021cce-880b-42b1-a09d-6d2c2222124e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"The ipython_unittest extension is already loaded. To reload it, use:\n  %reload_ext ipython_unittest\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["The ipython_unittest extension is already loaded. To reload it, use:\n  %reload_ext ipython_unittest\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Subtask 2: writing a few queries\nWrite the following queries in SQL to be executed by Spark in the next cell.\n\n1. 'q1': find the 'Id' of the most recently created post ('df' is 'posts_df') \n2. 'q2': find the number users\n3. 'q3': find the 'Id' of the user who posted most number of answers\n4. 'q4': find the number of questions\n5. 'q5': find the display name of the user who posted most number of comments\n\nNote that 'q1' is already available below as an example. Moreover, remmebr that Spark supports ANSI SQL 2003 so your queries have to comply with that standard."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7de10aba-7e77-4baa-af35-2b4e37916071"}}},{"cell_type":"code","source":["q1 = \"SELECT * FROM df ORDER BY CreationDate DESC limit 1\"\n\nq2 = \"SELECT COUNT(Id) FROM df\"\n\nq3 = \"SELECT OwnerUserId from df WHERE PostTypeId = 2 GROUP BY OwnerUserId ORDER BY COUNT(OwnerUserId) DESC LIMIT 1\"\n\nq4 = \"SELECT COUNT(Id) FROM df WHERE PostTypeId = 1\"\n\nq5 = \"SELECT df1.DisplayName FROM df1 JOIN df2 ON df1.Id = df2.UserId GROUP BY df2.UserId, df1.DisplayName ORDER BY COUNT(df2.UserId) DESC LIMIT 1\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c3273ef7-7f4d-4b5d-bcf7-9935c952e26d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Subtask 3: validating the implementations by running the tests\n\nRun the cell below and make sure that all the tests run successfully."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46400bac-c11c-4bee-81a8-67d42e3ca968"}}},{"cell_type":"code","source":["%%unittest_main\nclass TestTask2(unittest.TestCase):\n    def test_q1(self):\n        # find the id of the most recent post\n        r = run_query(q1, posts_df)\n        self.assertEqual(r, 95045)\n\n    def test_q2(self):\n        # find the number of the users\n        r = run_query(q2, users_df)\n        self.assertEqual(r, 91616)\n\n    def test_q3(self):\n        # find the user id of the user who posted most number of answers\n        r = run_query(q3, posts_df)\n        self.assertEqual(r, 64377)\n\n    def test_q4(self):\n        # find the number of questions\n        r = run_query(q4, posts_df)\n        self.assertEqual(r, 28950)\n\n    def test_q5(self):\n        # find the display name of the user who posted most number of comments\n        r = run_query2(q5, users_df, comments_df)\n        self.assertEqual(r, \"Neil Slater\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"31ca3fb9-427c-425d-b334-0df9c208a21b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+--------+----------+-------------------+-----+---------+--------------------+-----------+-------------------+-----+----+-----------+------------+-------------+---------+\n|   Id|ParentId|PostTypeId|       CreationDate|Score|ViewCount|                Body|OwnerUserId|   LastActivityDate|Title|Tags|AnswerCount|CommentCount|FavoriteCount|CloseDate|\n+-----+--------+----------+-------------------+-----+---------+--------------------+-----------+-------------------+-----+----+-----------+------------+-------------+---------+\n|95045|   95010|         2|2021-05-29 23:28:06|    0|        0|PHA+QXMgTmlrb3Mgc...|      64377|2021-05-29 23:35:16| null|null|          0|           0|            0|     null|\n+-----+--------+----------+-------------------+-----+---------+--------------------+-----------+-------------------+-----+----+-----------+------------+-------------+---------+\n\n95045\n+---------+\n|count(Id)|\n+---------+\n|    91616|\n+---------+\n\n91616\n+-----------+\n|OwnerUserId|\n+-----------+\n|      64377|\n+-----------+\n\n64377\n+---------+\n|count(Id)|\n+---------+\n|    28950|\n+---------+\n\n28950\n+-----------+\n|DisplayName|\n+-----------+\n|Neil Slater|\n+-----------+\n\nNeil Slater\nSuccess.....\n----------------------------------------------------------------------\nRan 5 tests in 17.899s\n\nOK\nOut[182]: <unittest.runner.TextTestResult run=5 errors=0 failures=0>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+--------+----------+-------------------+-----+---------+--------------------+-----------+-------------------+-----+----+-----------+------------+-------------+---------+\n|   Id|ParentId|PostTypeId|       CreationDate|Score|ViewCount|                Body|OwnerUserId|   LastActivityDate|Title|Tags|AnswerCount|CommentCount|FavoriteCount|CloseDate|\n+-----+--------+----------+-------------------+-----+---------+--------------------+-----------+-------------------+-----+----+-----------+------------+-------------+---------+\n|95045|   95010|         2|2021-05-29 23:28:06|    0|        0|PHA+QXMgTmlrb3Mgc...|      64377|2021-05-29 23:35:16| null|null|          0|           0|            0|     null|\n+-----+--------+----------+-------------------+-----+---------+--------------------+-----------+-------------------+-----+----+-----------+------------+-------------+---------+\n\n95045\n+---------+\n|count(Id)|\n+---------+\n|    91616|\n+---------+\n\n91616\n+-----------+\n|OwnerUserId|\n+-----------+\n|      64377|\n+-----------+\n\n64377\n+---------+\n|count(Id)|\n+---------+\n|    28950|\n+---------+\n\n28950\n+-----------+\n|DisplayName|\n+-----------+\n|Neil Slater|\n+-----------+\n\nNeil Slater\nSuccess.....\n----------------------------------------------------------------------\nRan 5 tests in 17.899s\n\nOK\nOut[182]: <unittest.runner.TextTestResult run=5 errors=0 failures=0>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Subtask 4: answering to questions about Spark related concepts\n\nPlease answer the following questions. Write your answer in one to two short paragraphs. Don't copy-paste; instead, write your own understanding.\n\n1. What is the difference between 'DataFrame', 'Dataset', and 'Resilient Distributed Datasets (RDD)'? \n2. When do you suggest using RDDs instead of using DataFrames?\n3. What is the main benefit of using DataSets instead of DataFrames?\n\nWrite your answers in the next cell."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fe8a2e0c-7e72-410b-b385-00503c0bc136"}}},{"cell_type":"markdown","source":["####1.\nLet's start with RDD, since they came first. A resilient distributed dataset is a collection of data that is distributed over several nodes. It is partitioned, and also replicated, which makes it faul-tolerant. Developers can perform transformations on an RDD, in which case a new copy is always made. DataFrames are also distributed collections of data, but the main difference from RDDs is that they use schemas. Every column in the dataFrame has its own name-identifier. The storage format can be e.g JSON or CSV. Another difference between RDDs and DataFrames, is that DataFrames comes with a built-in catalyst optimizer, while with RDDs, all optimization has to be done by the developers using them. Datasets can be seen as extensions of DataFrames. In addition to having name-identifiers for each column (a schema) and having a catalyst optimizer, Datasets also support type-checking very well. Also, Datasets have better object-oriented functionality. These extra features make Datasets slighly slower in aggregate operations compared with DataFrames, but it is still faster than RDDs.\n\n####2.\nRDDs should be used instead of DataFrames when working with unstructured data, that there is no point in imposing a schema on, since DataFrames require a schema. Also, RDDs are better when you want low-level actions and transformations, as DataFrames focuse more on higher-level abstractions and operations. Another benefit of RDDs is that the data can be manipulated with functional programming constructs, rather than having to use domain-specific expressions which is often the case with DataFrames. \n\n####3.\nThe main benefit of using DataSets instead of DataFrames is that with Datasets, you get a high degree of type-safety that you do not get with DataFrames."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c33ca21-1672-4c0c-a876-f73cbb8f65b2"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Task2","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":118798763941655}},"nbformat":4,"nbformat_minor":0}
