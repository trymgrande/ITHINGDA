# Assignment 6 - Individual Reflection Report

## IT3010 Research-based Innovation Methodologies in Computer and Information Science

### (Spring 2022) 

- The role of experiences and motivation in your research project. 

One experience we had while starting the project, was the consept of fake reviews. This consept has been a problem ever since reviews have been a thing, since companies want to make more money selling their products and/or services. Before the project, we have had experience with reviews, and sometimes even fake reviews. This has been annoying in my experience, as this can lead me to buy something I that I otherwise would not have, or that I get unsure of wether most of the reviews are real or fake. These reviews are often illegal, but hard to combat, as it can be hard to distinguish and automatically filter out everywhere easily and fast enough. This makes it a widespread problem both for humans and AI. 

<!-- In app stores, users can rate downloaded apps on a scale from 1 to 5 stars and write a review message. Thereby, they can express satisfaction ordissatisfaction, report bugs, or suggest new features. As a side effect, an illegal market for fake app reviews has emerged, with the goal to offer services that help app vendors improvetheir ratings and ranking in app stores. -->

Because of this, our motivation has therefore been to address this problem by evaluating how large this gap is when comparing identical data between humans and AI in terms of accuracy. Our plan to approaching this was to use a dataset that had already been tested on existing algorithms. The result this would have, is to be able to see a direct comparison between humans and AI, using the same data source. 




- The role of research literature and conceptual frameworks in your project. 

The role of the research litterature has mainly been looking at what findings have been made in the past regarding different machine learning algorithms that can effectively detect fake reviews. The most important requirement for selecting a paper to compare with here, was being able to obtain a sample of the dataset that was used. This has been very beneficial, as it allowed us to dive deeper by utilizing a lot of relevant data. 

At a high level, the main conceptual framework that was used was "The 6Ps of Research". This consists of 6 aspects that need to be considered in any research project.

Purpuse:
The purpuse is the reason for doing the research, and why it is important. As mentioned earlier, our purpuse has been to compare humans to AI in terms of accuracy of classification of fake reviews. As for the importance of this, it could have been beneficial to compare the two in terms of specific features to get a better insight. This is something that was critizised, but that we did not get time to go back and change.

Products:
The product is the outcome of the research, meaning the knowledge gained and the answers to the research question(s). (see [how your results address your research questions]).

Process:
The research process involves determining research topic(s) by establishing framework(s), research strategy, data generation methods, data analysis, and drawing conclusions.
This process started with a lot of brainstorming during group meetings. There, we discussed different approaches, along with their pros and cons. Once we figured out what to do, it was divided into smaller tasks that each of us could do on our own. This worked effectively, and allowed us to be flexible with our time once we agreed on something.


- The process of creating research questions, and the role research questions played in selecting a 
research strategy. 


Data collection
The data we needed was simply a binary classification for each of the review. This meant that we decided on a survey in order to get as many samples as possible. 

The dataset sample we received consisted of 16 000 reviews, that we randomly sampled into 15 reviews. This sample size could have been larger, but would take too long for each of the qustioneers. Alternatively, it could 



- The process of selecting a research strategy and data generation methods. For instance, why did 
you chose a specific strategy and data generation method combination, and what do you think 
about your choice now? 

On second thought, it might would have been better to use the data differently. Rather than simply comparing the accuracy, one could look at different features that humans specifically did worse at classifying. This would cover a wider knowledge gap, and could be more relevant in future work.



- Your data analysis process and how your results address your research questions.

The data analysis process was done using excel. By exporting the survey results from Nettskjema, and imporgin them into excel, we were able to analyze each of the classified reviews for each of the questioneers. This was then used to calculate a recall, precision, and accuracy value for each of the participants. These values were then used to calculate corresponding average values among all participants. 

Additionally, we received the duration of participation for each of the questioneers. This was analyzed by making sure that the average time spent was not too short.

when looking at the findings, we can see that there is an absolute difference between humans and AI of 49% precision and 46% recall. Therefore, when answering the RQ, we can say that this confirms the previously found results others have measured, in that humans are generally way worse than AI, even when looking at strictly the same data source, which is the point being made here.​


As for use of the results, since the gap between humans and AI has been confirmed to be relatively large in this case, the knowledge of this gap can be used to develop tools for teaching humans ways of improving this "classification" in order to help people from falling for things like this themselves.​

Further work can be done by primarily expanding the question sample size while potentially still using the same dataset. This will allow for better, and more accurate results. This can be done both by including more questioneers, but also by including a wider variety of samples from the dataset instead of reusing questions for every questioneer. Optimally, the entire data set would be used if enough questionaires are participating. In our survey, we used 41 questioneers with 15 questions each, adding up to a sample size of 615 total questions, meaning there is room for improvement.​